{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e7772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English utterances: 11843\n",
      "         file        group utterance\n",
      "0  028(1).cha  monolingual  better .\n",
      "1  028(1).cha  monolingual   uhhuh !\n",
      "2  028(1).cha  monolingual   uhhuh .\n",
      "3  028(1).cha  monolingual    mmhm .\n",
      "4  028(1).cha  monolingual         .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pylangacq as pla #program that reads the .cha files\n",
    "import pandas as pd #will organize the data into an early readable DataFrame\n",
    "import os #reads file directory paths\n",
    "\n",
    "CHA_DIR = \"/Users/kennedycameron/Downloads/Hoff/\"\n",
    "\n",
    "files = [os.path.join(CHA_DIR, f) for f in os.listdir(CHA_DIR) if f.endswith(\".cha\")]\n",
    "#go to the folder that contains the cha files and collect them\n",
    "\n",
    "utterances = []\n",
    "for f in files:\n",
    "    chat = pla.read_chat(f)\n",
    "    name = os.path.basename(f)\n",
    "    # loop through the files\n",
    "\n",
    "    group = \"monolingual\" if \"(1)\" in name else \"bilingual\"\n",
    "    #seperating monolingual and bilingual cha files\n",
    "\n",
    "    for utt in chat.utterances(participants=\"CHI\"):\n",
    "        lang = getattr(utt, \"language\", None)\n",
    "        if lang is None or lang == \"eng\":  \n",
    "            text = \" \".join(tok.word for tok in utt.tokens)\n",
    "    #pay attention to when the child is speaking, check if they are speaking english\n",
    "            utterances.append({\n",
    "                \"file\": name,\n",
    "                \"group\": group,\n",
    "                \"utterance\": text\n",
    "            })\n",
    "            # storing each usuable English utterance in a list\n",
    "\n",
    "\n",
    "df = pd.DataFrame(utterances)\n",
    "#convert that list to a dataframe for easy viewing\n",
    "\n",
    "#shows how much data we have captures and the head of the dataframe itself\n",
    "print(\"Total English utterances:\", len(df))\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#spaCy's English language model provides part of speech tagging and depenceny parsing so I used that to detect subejects within sentences.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#This function analyzes each utterance by\n",
    "def analyze_subject(utt):\n",
    "    doc = nlp(utt)\n",
    "\n",
    "    has_finite_verb = False\n",
    "    has_subject = False\n",
    "\n",
    "#(a) checking for a finite verb\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"VERB\", \"AUX\"} and token.morph.get(\"VerbForm\") != [\"Inf\"]:\n",
    "            has_finite_verb = True\n",
    "#(b) checking for a grammatical subject\n",
    "        if token.dep_ in {\"nsubj\", \"nsubj:pass\"}:\n",
    "            has_subject = True\n",
    "#(c) reporting the findings\n",
    "    return pd.Series({\n",
    "        \"has_finite_verb\": has_finite_verb,\n",
    "        \"has_subject\": has_subject\n",
    "    })\n",
    "\n",
    "#This function is applied to every utterance\n",
    "subject_info = df[\"utterance\"].apply(analyze_subject)\n",
    "df_analysis = pd.concat([df, subject_info], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11151bad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a4f8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file               object\n",
      "group              object\n",
      "utterance          object\n",
      "has_finite_verb      bool\n",
      "has_subject          bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_analysis = df_analysis.loc[:, ~df_analysis.columns.duplicated()]\n",
    "\n",
    "print(df_analysis.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4f17c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite clauses: 2146\n",
      "\n",
      "Summary stats:\n",
      "             total_clauses  omissions  omission_rate\n",
      "group                                               \n",
      "bilingual              760        131       0.172368\n",
      "monolingual           1386         93       0.067100\n"
     ]
    }
   ],
   "source": [
    "finite_df = df_analysis[df_analysis[\"has_finite_verb\"]].copy()\n",
    "finite_df[\"subject_omitted\"] = ~finite_df[\"has_subject\"]\n",
    "\n",
    "print(\"Finite clauses:\", len(finite_df))\n",
    "\n",
    "summary = finite_df.groupby(\"group\")[\"subject_omitted\"].agg(\n",
    "    total_clauses=\"count\",\n",
    "    omissions=\"sum\"\n",
    ")\n",
    "summary[\"omission_rate\"] = summary[\"omissions\"] / summary[\"total_clauses\"]\n",
    "\n",
    "print(\"\\nSummary stats:\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
